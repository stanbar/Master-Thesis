\chapter{Content Poisoning}
\label{content-poisoning}
First, we narrow the area of research in this thesis. Content Poisoning in NDN (and possibly in all ICN networks) can be divided into two categories: 1. Corrupted Data, and 2. Fake Data. In both of them, content gets modified. The first one assumes that an attacker does not possess a signing-key of an authentic publisher; thus, the content will not pass a signature verification process. It might look like the problem is resolved. Unfortunately, signature verification is an optional process, and there are two reasons why this process might not be done on every router. The first one is economic incentivization. The process of signature verification is resource-consuming; thus, businesses might decide to skip it. The other reason is that a signature verification requires a corresponding public key. We cannot expect from router to store all possible public keys. Thus the routers are vulnerable to Corrupted Data CPA. This kind of attacks has been addressed in papers \cite{ghali2014needle} \cite{yu2018content} \cite{nguyen2017content}. 

The second CPA category––Fake Data––is more dangerous because it assumes that an attacker gets access to the publisher's private key. Therefore he can forge a valid signature, which will successfully pass verification on an end-system. 

Once a bogus content gets propagated across multiple nodes (more specifically on their Content Stores), each end-user who requests content from that node receives the poisoned content. Signature verification passes successfully; hence a user is convinced that the content is authentic. Moreover, the content cannot be as easily revoked as it was disseminated because most ICN networks do not allow removing/revoking its cache content, other than natural (e.g., LRU based) cache aging. The one way to effectively prevent fetching fake content is by explicitly filtering the name of the content we consider poisoned. However, this raises another problem, how a user can know the name of the poisoned content, and if it is not too late. In \cite{nguyen2017content}, we can read, "Therefore, it (Fake Data CPA) is harder to create but impossible to detect by end-systems." At the time of writing, the problem is still unsolved, but there is one proposal \cite{konorski2019mitigating} which we research, expand, and modify. Before that, we will briefly review how other content-oriented services solve the Fake Data CPA problem.

\section{Wikipedia}
Wikipedia is an example of a system that faces a content poisoning problem. Let us investigate how do they solve the problem. 

"Wikipedia is a free online encyclopedia, created and edited by volunteers around the world and hosted by the Wikimedia Foundation."
Everyone can become a Wikipedia editor just by creating a free account. Account registration does not even require an email address. An editor can create a new article or modify an existing one. Before a change is publicly available, other editors read the proposition and have a chance to reject it or propose a further edit. If they cannot achieve consensus, they start a discussion and exchange their arguments. Finally, if they agree on one version, a consensus is achieved, and the new version of an article is published (keeping the whole history of changes).
Such ease of account creation makes Wikipedia vulnerable to Sockpuppeting (also known as Sybil attack). This vulnerability allows one user to create multiple accounts with different identities. Consequently, one person controls fake "public opinion," which can support his position in edits discussions. That is why a raw voting system is a non-preferred method in conflict resolutions.
Wikipedia fights content poisoning attacks by using humans to detect bogus changes. This mechanism seems to work well in such a service, but it does not scale to a public Internet protocol, so cannot be used as is.

\section{LOCKSS}
LOCKSS is a decentralized p2p digital preservation system for libraries, developed at the Standford University \cite{maniatis2003preserving}. Currently, it is an open-source, decentralized system used by many institutions. It is helping them to maintain a digital collection of journals, articles, and books. If we abstract from a specific type of data the system is operating on, we notice some similarities with ICN networks that we believe are worth investigating. The main difference between LOCKSS and ICN is that the former is very conservative; it rather prevents than expedites change of data. Nevertheless, this property may be useful in the context of preventing bogus data diffusion. 
In LOCKSS, each participating library becomes a node in a p2p network. It runs software responsible for collecting new content from e-journal websites by a crawler, serving already stored materials to local readers, and cooperating with other nodes in preserving materials when they get damaged. 
Due to publishers' copyrights, no node has to redistribute its content blindly to other peers. Other nodes can help repair damaged or completely missed material only to nodes that previously proved its ownership. New materials can only be acquired directly from a publisher's website to libraries which paid for subscription. 
Cyclic polls detect damages. Each interested peer votes on a hash of an Archive Unit (AU), the smallest unit in which nodes identify content. Since each node consists of a different set of Archive Units, the protocol treats each AU independently. If it turns out that some node contains AU with a different hash than most of the poll, it starts a sequence of repairs. As a result, LOCKSS becomes a self-healing store of data and does not increase the risk of free-loading non-purchased content.
LOCKSS is designed in a way that does not rely on long-time public-key cryptography (a system that must operate for decades is eventually highly susceptible to private-key leakage). Since it does not rely on public-key cryptography, peer identity management is minimalist; therefore, such a system cannot rely on peer reputations.

\section{Social Media and Fake News}
Fake News is deliberate disinformative news that is hard to identify and can lead to destructive consequences if not mitigated in time. Social media are a perfect ecosystem for disseminating fake news; specially designed algorithms serve us content that we are likely to agree with. In \cite{zhou2018fake}, the authors point out why social media algorithms are very effective in fake news dissemination. Social media allow us to form like-minded people into groups more accessible than ever before. Such groups facilitate the Echo Chamber Effect, where each individual is surrounded by people who share and produce content that fits our current worldview. It leads to a segmented and polarized society, which is very prone to believing fake news that confirms current opinions, even if there is limited or no reason to believe it. 

Fake news is often written in a very provocative fashion, making such content more attractive for interaction; more interaction leads to higher dissemination, fostering even more interaction. While for honest publishers and valuable content, this feature is helpful, it becomes a hazardous tool in the hands of malicious publishers. Additionally, the low cost of creating new accounts makes it even easier for an attacker to initiate the snowball effect, using fake accounts. Bots can interact with people or even with other bots, creating fake social opinions.  

We notice that this model fits perfectly into our Content Poisoning Attack model. In some sense, we are facing a similar problem fake news does in social media: we want to allow valuable content to spread as fast as possible, while limiting malicious content dissemination to a minimum.


\subsection{Mitigating fake news}
\label{mitigating-certification-services}
One idea is to create centralized services that reveal known fake news. Each time someone is susceptible to some news, they can check if the news is not present in blacklisted news. This solution has some flaws. Attackers can publish honest news in such an "oracle" service, leading to revoking genuine news, which can be as harmful as publishing fake news. A better way would be to check many independent "oracle" services and arrive at a decision whether to believe based on how many of them are skeptical of such news.

Another idea would be to create certification services. Each news could be signed by services that decide which content is authentic. A user could decide which certification services he/she trusts, and therefore inherit the trust in the news signed by such services. This idea seems to have worked for decades in journalism. A reliable journal reader does not need to check each article if it is fake or not. He inherits each article's confidence because he trusts that the journal's reviewers have eliminated fake or low-quality content. Additionally, a journal publisher has no interest in publishing low-quality content because he/she has economic incentives to become as creditworthy a publisher as possible.
This idea has some downsides. The process of reviewing an article is prolonged and requires qualified human interaction. Although some of the work can be outsourced to artificial intelligence, it is still a complicated task. Additionally, a journal publisher can introduce censorship or favor some kind of content. In other words, this mechanism does not scale for general-purpose content certification.

Let us consider fake news detection in practice. If Alice forgets to log out from social media account on a public library computer, someone can submit content, a post, that says, "I do not want to live anymore the way everyone else is living, this rat race is not for me, from today I become homeless. Goodbye." Such content is an excellent candidate to become successful fake news mainly because it is created from an author's account, which gives it high credibility, but also because it contains some arguable truth that can make it trustworthy. Unfortunately, as we discussed in the previous certification services approach, this idea of content evaluation does not scale to the level we need. We notice that to decide if a post is fake or not, we can just wait. In this situation, the time is playing a crucial role, because if the post is deleted from Alice's account after a short period, we can be sure that Alice would never have posted it. On the other hand, if the post is present for a long time on her page, we can state with high probability that she wants it to be there, thus it is authentic content.

This approach also has its downsides, namely, it is slow. If a piece of information is urgent, like information that some country has started World War III, we do not have hours to wait until the information gets deleted or not. We need to act fast. In such situations, this approach is inefficient. But for less urgent content, this approach offers several useful features. It scales well--for each content publication, only one person is involved to remove the content (in the case when it is bogus). It is maintenance-free--by default, no one needs to do anything if the content is authentic. It is flexible--a reader can individually decide if the time from publication is long enough to trust such content or not. If the information is not very impactful, we can trust it from the first hour from the publication; otherwise, e.g., in the case of a banking webpage, we could wait a few hours until we enter the credentials into it. It is like HTTPS and HTTP, but much more flexible--some pages can be safely browsed over HTTP, while others should be used only over HTTPS. 

As discussed before, the fake news problem in a social network is very similar to the CPA problem in ICN networks. So the solutions that work in one domain could work in the other. In this thesis, we will continue the idea of a content time availability, which we will call \emph{Proof-of-Time}.